import boto3  						# Importing boto3 for AWS S3 interaction
import os 						# Importing os to handle environment variables
from concurrent.futures import ThreadPoolExecutor  	# Importing ThreadPoolExecutor to manage parallel tasks

s3_client = boto3.client('s3')  

def copy_object(source_bucket, destination_bucket, file_key):
    copy_source = {'Bucket': source_bucket, 'Key': file_key}  					# Defining source bucket and object key
    s3_client.copy_object(CopySource=copy_source, Bucket=destination_bucket, Key=file_key)  	# Copying the object to the destination bucket

def lambda_handler(event, context):
    source_bucket = os.getenv('SOURCE_BUCKET')  						# Getting the source bucket name from environment variables
    destination_bucket = os.getenv('DESTINATION_BUCKET')  					# Getting the destination bucket name from environment variables
    prefix = 'weather/'  									# Defining the prefix (folder) to look for in the source bucket
    continuation_token = None  									# Initializing continuation token for pagination in case there are multiple 												#pages of results
    response = s3_client.list_objects_v2(Bucket=source_bucket, Prefix=prefix, Delimiter='/')  	# Listing objects under the specified prefix

    if 'CommonPrefixes' in response:  								# Checking if there are common prefixes (subfolders) under the specified 												#prefix
        with ThreadPoolExecutor(max_workers = 150) as executor:  				# Using a ThreadPoolExecutor to process files in parallel
            for prefix_info in response['CommonPrefixes']:  					# Iterating through each subfolder (prefix)
                prefix = prefix_info['Prefix']  						# Extracting the prefix (subfolder path)
                if 'Novi Sad' in prefix:  							# Filtering out only the prefixes related to Novi Sad
                    while True:  								# Loop to handle pagination of S3 objects if there are multiple pages
                        if not continuation_token:  						# If no continuation token, fetch the first page of objects
                            objects = s3_client.list_objects_v2(Bucket=source_bucket, 
			    Prefix=prefix)
                        else:  									# If there is a continuation token, fetch the next page of objects
                            objects = s3_client.list_objects_v2(Bucket=source_bucket, 
			    Prefix=prefix, ContinuationToken=continuation_token)
                        if 'Contents' in objects:  								# If there are objects in the current page
                            for obj in objects['Contents']: 							# Iterating over each object
                                file_key = obj['Key']  								# Extracting the object key (file path)
                                executor.submit(copy_object, source_bucket, destination_bucket, file_key)  	# Submitting the copy task to the executor
                        continuation_token = objects.get('NextContinuationToken')  				# Updating the continuation token for the next page
                        if not continuation_token:  								# If there is no continuation token, stop the loop
                            break
    return {'statusCode': 200, 'body': f"Transferred files from {source_bucket}/{prefix} to {destination_bucket}"}  # Returning success response
